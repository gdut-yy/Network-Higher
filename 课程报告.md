# 使用 Docker 来模拟真实的网络

## 1 Docker 是什么
Docker 是一个允许用户 “在任何地方构建、分发及运行任何应用” 的平台。它在极短的时间内发展壮大，目前已经被视为解决软件中最昂贵的方面之一 —— 部署的一个标准方法。

在 Docker 出现之前，开发流水线通常有用于管理软件活动的不同技术组合而成，如虚拟机、配置管理工具、不同的包管理系统以及各类依赖库复杂的网站。所有这些工具需要由专业的工程师管理和维护，并且多数工具都具有自己独特的配置方式。

Docker 改变了这一切，允许不同的工程师参与到这个过程中，有效地使用同一门语言，这让写作变得轻而易举。所有东西通过一个共同的流水线转变成可以在任何目标平台上使用的单一的产出——无序继续维护一堆让人眼花缭乱的工具配置。

与此同时，只要现存的软件技术栈依然有效，用户就无需抛弃它——用户可以将其原样打包到一个 Docker 容器内供其他人使用。由此获得的额外好处是，用户能看到这些容器是如何构建的，因此如果需要深入其细节，完全没问题。
## 2 Docker 有什么好处
为什么要使用 Docker，Docker 用在什么地方？针对 “为什么” 的简要答案是：只需要一点点付出，Docker 就能快速为企业节省大量金钱。

1. 替代虚拟机（VM）
	- Docker 可以在很多情况下替代虚拟机。如果用户只关心应用程序，而不是操作系统，可以用 Docker 替代虚拟机，并将操作系统交给其他人去考虑。Docker 不仅启动速度比虚拟机快，迁移时也更为轻量，同时得益于它的分层文件系统，与其他人分享变更时更简单、更快捷。而且，它牢牢地扎根在命令行中，非常适合脚本化。
2. 软件原型
	- 如果想快速体验软件，同时避免干扰目前的设置或配备一个虚拟机的麻烦，Docker 可以在几毫秒内提供一个沙箱环境。
3. 打包软件
	- 因为对于 Linux 用户而言，Docker 镜像实际上没有任何依赖，所以非常适合用于打包软件。用户可以构建镜像，并确保它可以运行在任何现代 Linux 机器上——就像 Java 一样，但不需要 JVM。
4. 让微服务架构称为可能
	- Docker 有助于将一个复杂系统分解成一系列可组合的部分，这让用户可以用更离散的方式来思考其服务。用户可以在不影响全局的前提下重组软件使其各部分更易于管理和可插拔。
5. 网络建模
	- 由于可以在一台机器上启动数百个（甚至数千个）隔离的容器，因此对网络进行建模轻而易举。这对于现实世界场景的测试非常有用，而且所费无几。
6. 离线时启用全栈生产力
	- 因为可以将系统的所有部分捆绑在 Docker 容器中，所以用户可以将其编排运行在笔记本电脑中移动办公，即便离线时也没问题。
7. 降低调试支出
	- 不同团队之间关于软件交付的复杂谈判在业内司空见惯。我们亲身经历过不计其数的这类讨论：失效的库、有问题的依赖、更新被错误实施或是执行顺序有误，甚至可能根本没执行以及无法重现的错误等。估计读者也遇到过。Docker 让用户可以清晰地说明（即便是脚本的形式）在一个属性已知的系统上调试问题的步骤，错误和环境重现变得更简单，而且通常与所提供的宿主环境是分离的。
8. 文档化软件依赖及接触点
	- 通过使用结构化方式构建镜像，为迁移到不同环境做好准备，Docker 强调用户从一个基本出发点开始明确地记录软件依赖。即使用户不打算在所有地方都使用 Docker，这种对文档记录的需要也有助于在其他地方安装软件。
9. 启用持续交付
	- 持续交付（continuous delivery，CD）是一个基于流水线的软件交付泛型，该流水线通过一个自动化（或半自动化）流程在每次变动时重新构建系统然后交付到生产环境中。
	- 因为用户可以更准确第控制构建环境的状态，Docker 构建比传统软件构建方法更具有可重现性和可复制性。吃持续交付变得更容易。通过实现一个以 Docker 为中心的可重现的构建过程，标准的持续交付技术，如蓝/绿部署（blue/green deployment，在生产环境中维护 “生产” 和 “最新” 部署）和凤凰部署（Phoenix deployment，每次发布时都重新构建整个系统）变得很简单。

## 3 使用 Docker 来模拟真实世界的网络

多数用户会将互联网当做一个黑盒看待，，即通过某些方式从世界其他信息并显示在屏幕上。是不是会碰到网速慢或连接中断的情况，对 ISP 的抱怨屡见不鲜。

咋构建的镜像包含需要进行连接的应用程序时，用户可能对哪些组件需要连接到哪里以及整体的设置情况有一个清晰的了解。但有一件事是不变的：还是会遭遇网速慢和连接中断的情况。即是食用油并运营者自由数据中心的大型公司，也能观察到不可靠的网络及其引起的应用程序问题。一下是几种方法，对不可靠网络进行体验，已确定现实世界中可能面对的问题。
### 3.1 使用 Comcast 模拟有问题的网络

尽管用户在进行跨多主机分发应用程序时虚妄网络状况尽可能好，但现实却是残酷的——分组（packet，也称数据包）丢失（也称丢包）、连接中断、网络分区比比皆是，尤其是咋商用云服务供应商上。

在技术栈遭遇现实世界的这些情况之前对其进行测试以确认其行为是非常明智的——一个为高可用设计的应用程序不应该在玩不服务开始出现显著的额外实验时陷入停顿。 

问题：想要为单个容器应用不同的网络状况。

解决方案：使用 Comcast（指的是网络工具，而非 ISP）。

Comcast（https://github.com/tylertreat/comcast） 是一个娱乐化命名的工具，用于修改 Linux 机器的网络接口，以便对其应用某些不同寻常的（或者，对不走运的人而言是典型的）状况。

在 Dokcer 创建容器时，它会同时创建几个虚拟的网络接口——这也是容器具有不同 IP 并且可以相互通信的原因。因为这些都是标准网络接口，只要能查找出其网络接口名称，就可以在其上使用 Comcast。这说起来容易做起来难。

以下 Docker 镜像包含了 Comcast 及其前置要求，以及一些优化：

	$ IMG=dockerinpractice/comcase
	$ docker pull &IMG
	latest: Pulling from dockerinpractice/comcast
	[...]
	Stautus: Download newer image for dockerinpractice.comcast:latest
	$ alias comcase="docker run --rm --pid=host --privileged" \
	-v /var/run/docker.sock:/var/run/docker.sock $IMG"
	$comcast -help
	Usage of comcast:
	  -cont="": Container ID or name to get virtual interface of
	  -default-bw=-1: Default bandwidth limit in kbit/s (fast-lane)
	  -device="": Interface (device) to use (default to eth0 where applicable)
	  -dry-run=false: Specifies whether or not to commit the rule changes
	  -latency=-1: Latency to add in ms
	  -mode="start": Start or stop packet controls
	  -packet-loss="0": Packet loss percentage (eg: 0.1%%)
	  -target-addr="": Target addresses, \
	(eg: 10.0.0.1 or 10.0.0.0/24 or 10.0.0.1,192.168.0.0/24)
	  -target-bw=-1: Target bandwidth limit in kbit/s (slow-lane)
	  -target-port=-1: Target port(s) (eg: 80 or 1:65535 or 22,80,443,1000:1010)
	  -target-proto="tcp,udp,icmp": \
	Target protocol TCP.UDP (eg. tcp or tcp,udp or icmp)

新增的优化提供了 -conf 选项，可以指向一个容器而无须查找虚拟网络接口的名称。请注意，为了赋予容器更多权限，docker run 命令中曾家乐一些特殊的标志，这样 Comcast 就可以自由地对网络接口进行检查并应用变更。

为了展示 Comcast 可以带来的变化，先来看一下一个正常的网络连接是什么样的。打开一个新的终端，并运行一下命令来设置基准网络性能的预期：

	$ docker run -it --name cl ubuntu 14.04.2 bash
	root@0749a2e74a68:/# apt-get update && apt-get install -y wget
	[...]
	root@0749a2e74a68:/# ping -1-c 5 www.docker.com
	PING www.docker.com (104.239.220.248) 56(84) bytes of data.
	
	--- www.docker.com ping statistics ---
	5 packets transmitted, 5 received, 0% packet loss, time 4005ms
	rtt min/avg/max/mdev = 98.546/101.272/106.424/2.880 ms
	root@0749a2e74a68:/# time wget -o /dev/null httrps://www.docker.com
	
	real	0m0.680s
	user	0m0.012s
	sys		0m0.006s
	root@0749a2e74a68:/# 

完成上述步骤后，保持该容器处于运行状态，然后对应用一些网络状况：

	$ comcast-cont cl -default-bw 50 -latency 100 -packet-loss 20% 
	2019/01/03 02:28:13 Found interface vetha7b90a7 for container 'c1'
	sudo tc qdisc show | grep "netem"
	sudo tc qdisc add dev vetha7b90a7 handle 10: root htb
	sudo tc class add dev vetha7b90a7 parent 10: classid 10:1 htb rate 50kbit
	sudo tc class add dev vetha7b90a7 parent 10: classid 10:10 htb rate 50kbit
	sudo tc class add dev vetha7b90a7 parent 10: classid 100:netem delay 100ms loss 20.00%
	sudo iptables -A POSTROUTING -t mangle -j CLASSIFY --set-class 10:10 -p tcp
	sudo iptables -A POSTROUTING -t mangle -j CLASSIFY --set-class 10:10 -p udp
	sudo iptables -A POSTROUTING -t mangle -j CLASSIFY --set-class 10:10 -p icmp
	2019/01/03 02:28:13 Packet rules setup...
	2019/01/03 02:28:13 Run `sudo tc -s qdisc` to double check
	2019/01/03 02:28:13 Run `comcast --mode stop` to reset

上述命令应用了3种不同的状况：针对所有目标设置 50 KB/s 的带宽上限（唤起了对拨号的回忆），添加 100ms 的延迟，以及 20% 的分组丢失率。

Comcast 首先确定容器正确的虚拟网络接口，然后调用一些标准的 Linux 命令行网络工具来应用流量规则，并在执行过程中列出其所作的动作。来看一下容器是如何对此进行回应的：

	root@0749a2e74a68:/# ping -1-c 5 www.docker.com
	PING www.docker.com (104.239.220.248) 56(84) bytes of data.
	
	--- www.docker.com ping statistics ---
	5 packets transmitted, 5 received, 0% packet loss, time 4005ms
	rtt min/avg/max/mdev = 98.546/101.272/106.424/2.880 ms
	root@0749a2e74a68:/# time wget -o /dev/null httrps://www.docker.com
	
	real	0m9.673s
	user	0m0.011s
	sys		0m0.011s

成功了！ping 报告的延迟增加了 100ms，而对 wget 的计时展示了10倍左右的降速，与预期相当（带宽上限、额外的延迟及分组丢失同时产生了影响）。但是分组丢失有点儿奇怪——似乎比预期代理3倍。需要注意的很重要的一点是，ping 只发送了少量的分组，而分组丢失不是精确的 “五分之一” 计数器——如果将 ping 次数提高到 50，将会发生分组丢失结果与预期要接近得多。

注意，上面应用的规则对通过该网络接口的所有网络连接都有效，包括与宿主机及其他容器的连接。

现在告诉 Comcast 删除这些规则。Comcast 还无法对单个状况进行添加或删除，因此修改某张网络接口上的任何东西意味着要完全删除或重新添加该网络接口上的规则。如果要恢复正常的容器网络操作，也必须删除这些规则。不过，在退出容器时无须考虑这些规则的删除——它们会在 Docker 删除虚拟网络接口时被自动删除：

S

## 使用 Blockade 模拟有问题的网络
对很多应用程序而言，Comcast 是一个优秀的工具，不过有一个重要的使用场景它无法解决——如何将网络状况应用到全体容器中？手工对几十个容器运行 Comcast 僵尸非常痛苦的，上百个更是无法想象！这个问题对容器而言尤为相关，因为它们的启动代价非常低——如果在单台机器上运行上百个虚拟机而不是容器来模拟大型网络，将会遇到更大的问题，如内存不足！

在使用多台机器模拟网络时，有一张特殊类型的网络故障会在这种规模下变得有趣起来——网络分区。当一组网络化的机器被分成两个或更多部分时，这种情况就会出现，同一部分里的所有机器可以互相通信，但不同部分则无法通信。研究表明这种情况的发生比想象中要多得多，尤其是在消费级的云服务上！

遵循经典的 Docker 微服务线路可急剧缓解此类问题，而要理解服务如何对其进行处理，用友用于做实验的工具就至关重要了。

问题：想要对大量容器进行网络状况编排设置，包括创建网络分区。

解决方案：使用 Blockade。

Blockade 是出自戴尔公司一个部门的开源软件，为 “测试网络故障及分区” 而生。Blockade 通过读取当前目录中的配置文件（blockade.xml）来工作，该配置文件定义了容器启动的方式以及需要对其应用的状况。完整的配置细节可从 Blockade 文档中获得，因此这里只对核心部分进行说明。

首先需要创建一个 blockade.yml：

	containers:
	  server:
	    image: ubuntu:14.04.2
	    command: /bin/sleep infinity
	    expose: [10000]
	  client1:
	    image: ununtu :14.04.2
	    command: sh -c "ping $SERVER_PORT_10000_TCP_ADDR"
	    links: ["server"]
	  client2:
	    image: ununtu :14.04.2
	    command: sh -c "ping $SERVER_PORT_10000_TCP_ADDR"
	    links: ["server"]
	
	  network:
	    flaky: 50%
	    slow: 100ms

上述配置中设置的容器代表有两个客户端连接的一个服务器。在实践中，可能是一个数据库服务器及其客户端应用程序，并且对咬建模的组件数量没有任何限制。只要它能在一个 compose.yml 文件中表示，就可以在 Blockade 中对其进行建模。

在 server 的配置中指定了要暴露的一个端口，但并未在其上提供服务也不会对其进行连接——这这是启用了 Docker 的链接功能，并在客户端容器中暴露相关的环境变量以便让它们知道去 ping 哪个 IP。如果使用了其他 IP 发现技巧，则这些链接就不是必需的。

与往常一样，使用 Blockade 的第一步是拉取镜像：

	$ IMG=dockerinpractice/blockade
	$ dicker pull $IMG
	latest: Pulling form dockerinpractice/blockade
	[...]
	Status: Downloaded newer image for dockerinpractice/blockade:latest
	$ alias blockade="dockerrun -rm --pid=host --privileged \
	-v \$PWD:/blockade -v /var/run/docker.sock:/var/run/docker.sock $IMG"

可以看到，这里传递给 docker run 的参数与上一各的参数一致，只有一个例外 —— Blockade 将当前目录挂载到容器中以便访问 blockade.yml，并将状态存储在一个隐藏目录中。

最后到了关键时刻——运行 Blockade。要确保目前位于保存 blockade.yml 的目录中：

	$ blockade up
	NODE	CONTAINER ID	STATUS	IP			NETWORK	PARTITION
	client1	8c4d956cf9cf	UP		172.17.0.53	NORMAL
	client2	fcd9af2b0eef	UP		172.17.0.54	NORMAL
	server	b8f9f179a10d	UP		172.17.0.52	NORMAL

所有配置文件中定义的容器已经启动，并显示了已启动容器的一些有用信息。现在来应用一些基本的网络情况。在一个新的终端中持续打印 client1 的日志（使用 docker logs -f 8c4d956cf9cf），以便在做修改时可以查看所发生的情况：

	$ blockade flaky --all
	$ sleep 5
	$ blockade slow client1
	$ blockade status
	NODE	CONTAINER ID	STATUS	IP			NETWORK	PARTITION
	client1	8c4d956cf9cf	UP		172.17.0.53	SLOW
	client2	fcd9af2b0eef	UP		172.17.0.54	FLAKY
	server	b8f9f179a10d	UP		172.17.0.52	FLAKY
	$ blockade fast -all

flaky 和 slow 命令使用了之前配置文件中 network 一节定义的值——限定值无法在命令行中指定。如果有需要，可以在容器运行时编辑 blockade.yml，然后有选择地将新的限定值应用到容器上。需要注意的是，一个容器只能处在慢速网络或不稳定网络中，不能二者皆有。撇开这些限制，对成百上千个容器运行这一命令的便捷性还是相当可观的。

如果回头查看 client1 的日志，可以看到不同命令生效的时间：

	64 bytes from 172.17.0.52: icmp_seq=638 ttl=64 time=0.054ms
	64 bytes from 172.17.0.52: icmp_seq=639 ttl=64 time=0.098ms
	64 bytes from 172.17.0.52: icmp_seq=640 ttl=64 time=0.112ms
	64 bytes from 172.17.0.52: icmp_seq=645 ttl=64 time=0.112ms
	64 bytes from 172.17.0.52: icmp_seq=652 ttl=64 time=0.113ms
	64 bytes from 172.17.0.52: icmp_seq=654 ttl=64 time=0.115ms
	64 bytes from 172.17.0.52: icmp_seq=660 ttl=64 time100ms
	64 bytes from 172.17.0.52: icmp_seq=661 ttl=64 time100ms
	64 bytes from 172.17.0.52: icmp_seq=662 ttl=64 time100ms
	64 bytes from 172.17.0.52: icmp_seq=663 ttl=64 time100ms

虽然这很有用，不过在 Comcast 之上使用一些（可能是比较费力的）脚本也能实现，那么来看看 Blockade 的杀手锏——网络分区：

	$ blockade partition server client1,client2
	$ blockade status
	NODE	CONTAINER ID	STATUS	IP			NETWORK	PARTITION
	client1	8c4d956cf9cf	UP		172.17.0.53	SLOW	2
	client2	fcd9af2b0eef	UP		172.17.0.54	FLAKY	2
	server	b8f9f179a10d	UP		172.17.0.52	FLAKY	1

这会将3个节点划分成两个区域——服务器在其中一个区域，而客户端在另一个区域，它们之间无法进行通信。可以看到 client1 的日志停止了，因为所有的 ping 分组都丢失了！不过，两个客户端依然可以相互通信，这一点可以通过二者之间发送一些 ping 分组来验证：

	$ docker exec 8c4d956cf9cf ping -qc 3 172.17.0.54
	PING 172.17.0.54 (172.17.0.54) 56(84) bytes of data.
	
	--- 172.17.0.54 ping statistics ---
	3 packets transmitted, 3 received, 0% packet loss, time 1999ms
	rtt min/avg/max/mdev = 0.065/0.084/0.095/0.015ms

没有分组丢失，延迟也很低······看起来是个不错的连接。分区及其他网络状况是独立操作的，因此可以在应用分区的同时试验分组丢失。可以定义的分区数量没有限制，因此可以尽情地对复杂场景进行试验。

最后一个建议是，将 Blockade 和 Comcast 组合起来能获得比它们单独提供的能力更强大。Blockade 擅长创建分区以及完成启动容器的繁杂事务，添加 Comcast 则能实现每一个容器网络连接的细粒度控制！